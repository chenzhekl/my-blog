---
title: "[PRML翻译] 1. 导论"
date: 2018-06-13T22:44:19+09:00
draft: true
---

在数据中寻找规律是一个基本，且拥有悠久、成功历史的问题。举例来说，16 世纪第谷·布拉赫庞大的天文观测数据，使得约翰内斯·开普勒得以发现行星运动的经验规律，这反过来又为经典力学的发展提供了跳板。类似的，原子光谱规律的发现，在 20 世纪早期量子力学的发展和验证中扮演了关键角色。模式识别旨在通过计算机算法，自动地发现数据中存在的规律，并利用这些规律完成诸如数据分类的工作。

考虑手写数字识别这个例子（见图 1.1）。每个数字对应一幅 `$ 28 \times 28 $` 像素的图片，因此可以表示为包含 `$ 784 $` 个实数的向量 `$ \mathbf{x} $`。我们的目标是构建一台机器，以向量 `$ \mathbf{x} $` 为输入，输出对应的数字 `$ 0, \dots, 9 $`。这个问题并不简单，因为同一个数字的手写体间存在巨大差别。我们可以基于笔画的形状手工编写规则进行识别，但在实践中这往往需要大量的规则和例外，并且总是生成糟糕的结果。

{{< figure src="/posts/2018/06/14/figure1.1.png" title="图 1.1" caption="美国邮政编码的手写数字示例。" width="100%" >}}

要取得更好地结果，我们可以使用机器学习的方法，把 N 张手写数字的图片`$ \{\mathbf{x}_1, \dots, \mathbf{x}_N\} $` 作为 _训练集_ ，来调整一个适应性模型的参数。每张图片对应的数字，通常通过手工标注的方式预先知晓。我们可以图片把对应的数字（即类别）以 _目标向量_ `$ \mathbf{t} $` 表达。之后会详细讨论使用向量表达类别的方法。注意，每张图片 `$ \mathbf{x} $` 对应一个向量 `$ \mathbf{t} $`。

机器学习学的的结果可以以一个函数 `$ \mathbf{y}(\mathbf{x}) $` 表达，其接收一张数字图片 `$ \mathbf{x} $` 作为输入，输出一个向量 `$ \mathbf{y} $`，向量 `$ \mathbf{y} $` 采用和目标向量相同的编码方式。函数 `$ \mathbf{y}(\mathbf{x}) $` 的具体形式在 _训练_ 阶段（也称为 _学习_ 阶段）被确定，训练的结果取决于训练集。一旦确定模型训练好，就可以用于确定数字图片的类别，这些图片对应的组成 _测试集_ 。能够正确分类不存在于训练集中的图片，这种能力称之为 _泛化_ 。在实际应用中，由于输入向量的千差万别，训练集往往只能包含所有可能向量的很小一部分，因此泛化能力是模式识别中一个主要目标。

对大多数实际应用来说，输入变量会经过 _预处理_ ，转化为新空间中的变量，以便模式识别问题能够更简单的得到解决。举例来说，在数字识别问题中，数字图片通常会被平移、拉伸成指定大小。因为位置和尺寸都得到归一化，这么做显著减少了每类数字图片中可能存在的变化，也使得接下来的模式识别算法能更简单的区别不同类别的图像[^1]。预处理阶段有时也被称为 _特征提取_ 。注意，新的测试数据必须经过和训练数据一样的预处理过程。

[^1]: 译注：即减少类内差别，间接使得类间差别相对类内差别更加明显，因而简化分类。

预处理有时也被用于加速计算过程。举例来说，如果问题目标是在高分辨率视频流中进行实时人脸识别，计算机必须每秒处理大量的像素，将这些像素直接交给复杂的模式识别算法，通常在计算上不可行。取而代之，我们寻找一些可以快速计算的特征，这些特征依然保留有区别人脸与非人脸的判别信息。它们随后被送进模式识别算法。举个例子，图片矩形子区域内的平均像素值可以被很高效的计算（Viola and Jones, 2004），这样一组特征被证实在快速人脸检测中有很好的效果。因为平均像素值的数量比原始像素少很多，这种预处理代表了降维的一种手段。预处理必须很小心，因为在这过程中往往会丢弃信息，如果这些信息对解决问题很重要，系统的总体精确度就会受到影响。

如果训练数据包含同时包含输入向量和对应的目标向量，这种设定被称为 _监督学习_ 问题。一个例子是数字识别，其目标是从有限的离散类别中，给每个输入向量确定一个，这也被称作 _分类_ 问题。如果输出包含一个或多个连续变量，则被称作 _回归_ 问题。一个例子是给定反应物的浓度，温度与压力，预测化学反应过程的产量。

其他一类模式识别问题，其训练数据只包含一组输入变量 `$ \mathbf{x} $`，没有目标变量。这被称为 _非监督学习_ 问题，一个目标是将数据中的相似例子聚合成类，也被称为 _聚类_ ；或者确定数据在输入空间中的分布，被称作 _密度估计_ ；亦或是将数据从高维空间映射到二维或三维空间中，以便 _可视化_ 。

最后一类被称作 _强化学习_ （Sutton and Barto, 1998）的技术关注在给定环境中，寻找合适的操作以便使奖励最大化。和监督学习相反，这类学习算法不知道给定输入的最优输出，只能通过试错自己发掘。通常会给定一系列状态与操作，学习算法通过不同的操作与环境交互，并切换状态。在许多例子中，当前的操作不止影响即时奖励，也会对之后的所有步骤造成影响。举个例子，通过适当的强化学习算法，神经网络能够学会高水平的下西洋双陆棋（Tesaur, 1994）。其中，神经网络必须学会以骰子结果和棋盘状态为输入，输出一步强手。为了实现这个目标，神经网络自我对弈了数百万盘。西洋双陆棋中的一个挑战是，游戏涉及大量的回合数，但只有到最后才能得到输赢这个奖励。奖励必须被合理的分配到每一手中，即区别哪些操作时好的，那些是坏的。这也被称为 _信用分配_ 问题。强化学习中的一个关键是权衡 _探索_ 与 _遵从_ 。探索即系统尝试新的操作，观察是否有效；遵从即系统沿用已经被证明有效（产生高奖励）的操作。太过于偏向于探索或遵从都会导致很差的结果。强化学习是机器学习研究领域的热点。然而，展开讨论超出了本书的范畴。

虽然前述各种问题都有自己的工具和方法，但它们底层的想法是共通的。本章的一个主要目标是以非正式的形式，介绍其中一些最重要的概念，并附以简单例子。在随后的章节中，我们会看到相同的想法在更复杂的模型（足以应用到实际问题中）中重复出现。本章也介绍了随后会频繁使用的三个工具，即概率论、决策论和信息论。虽然这些主题可能听起来有些令人畏惧，它们实际上非常直接易懂，如果你想把机器学习技术用好，清晰地掌握它们是必不可少的。

## 例子：多项式曲线拟合

我们先介绍一个简单的回归问题，并用它作为本章的例子引出一些重要概念。假设我们观测到一个实数输入变量 `$ x $`，希望使用它预测一个实数目标变量 `$ t $` 的值。出于教学目的，我们考虑使用一组手工生成的数据，这样我们可以了解数据生成的具体过程，并将之与学得的模型进行比较。这个例子中的数据采用函数 `$ \sin(2\pi x)$` 生成，并在目标值中添加了随机噪声，详情见附录 A。

现在假设给定一个训练集，其中包含 `$ N $` 个观测 `$ x $`， `$ \boldsymbol{\mathsf{x}} \equiv (x_1, \dots, x_N)^\mathrm{T} $`，与对应的目标值 `$ t $`，记为 `$ \boldsymbol{\mathsf{t}} \equiv (t_1, \dots, t_N)^\mathrm{T} $`。图 1.2 展示了包含 `$ N = 10 $` 个数据点的训练集。图 1.2 中的输入数据集使用如下方法生成，等间距的在 `$ [0, 1] $` 上采样得到 `$ x_n $`，`$ n = 1, \dots, N $`，将每个 `$ x_n $` 送入函数 `$ \sin{(2\pi x)}$`，计算得到的值再加上一个服从高斯分布（高斯分布在 1.2.4 节介绍）的小幅噪声，得到对应的值 `$ t_n $`，组合起来获得 `$ \boldsymbol{\mathsf{t}} $`。通过这样的方法，我们模拟了许多真实数据集的一个特点，即实数集拥有一定的内在规律，这是我们希望学习的，但是其中独立的观测都变噪声破坏了。噪声可能来内在的随机过程，例如辐射衰变，更典型的情况是，来自未观测到的引起变化的原因。

{{< figure src="/posts/2018/06/14/figure1.2.png" title="图 1.2" caption="包含 `$ N = 10 $` 个数据点的训练集，显示为蓝色圆圈，每个数据点包含输入变量 `$ x $` 和对应的目标变量 `$ t $`。绿色曲线为生成数据所用的函数 `$ \sin{(2\pi x)}$`。我们的目标是在不知道绿色曲线的情况下，给定新的 `$ x $`，预测对应的 `$ t $`。" width="100%" >}}

我们的目标是利用训练集，针对新的输入变量值 `$ \hat{x} $`，预测对应的目标变量值 `$ \hat{t} $`。如我们之后会看到的，这隐式地涉及寻找底层函数 `$ \sin(2\pi x) $`。这是一个内在很难的问题，因为我们必须从有限的数据集中泛化。更进一步的，观测到的数据被噪声破坏了，因此对给定 `$ \hat{x} $`，`$ \hat{t} $` 的值存在一定的不确定性。1.2 节中讨论的概率论为精确、定量地表达不确定性提供了框架，1.5 节中介绍的决策论使得我们在给定条件下，可以利用概率表示做出最优的预测。

现在我们暂时非正式的，使用一个简单的方法完成曲线拟合。具体来说，我们使用下述形式的多项式函数拟合数据

<div>
\begin{equation}
y(x, \mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \dots + w_M x^M = \sum_{j = 0}^M{w_j x^j}
\end{equation}
</div>

其中 `$ M $` 是多项式的 _阶数_ ，`$ x^j $` 表示 `$ x $` 的 `$ j $` 次方。多项式系数 `$ w_0, \dots, w_M $` 合起来由向量 `$ \mathbf{w} $` 表示。注意，虽然多项式函数 `$ y(x, \mathbf{w}) $` 是 `$ x $` 的非线性函数，但它是系数 `$ \mathbf{w} $` 的线性函数。像多项式这样对于未知参数是线性的函数，它们有重要的性质，被称作 _线性模型_ ，我们会在第 3 和 4 章详细讨论。

系数的值由拟合多项式到数据的结果确定。这可以通过最小化 _误差函数_ 实现，误差函数量化函数 `$ y(x, \mathbf{w}) $`（给定 `$ \mathbf{w} $` 的值） 与训练集数据点间的误差。一个广泛使用的误差函数是平方和误差，给定数据点 `$ x_n $` 的预测 `$ y(x_n, \mathbf{w}) $`，及对应的目标值 `$ t_n $`，我们最小化

<div>
\begin{equation}
E(\mathbf{w}) = \frac{1}{2}\sum_{n = 1}^N{\{y(x_n, \mathbf{w}) - t_n\}}
\label{eq:sum-square}
\end{equation}
</div>

其中系数 `$ \frac{1}{2} $` 仅为后续计算方便。我们在本章稍候会讨论选择这个误差函数的动机。现在只需了解这是一个非负函数，当且仅当函数 `$ y(x, \mathbf{w}) $` 通过训练集的每个数据点时为 `$ 0 $`。平方和误差函数的几何解释见图 1.3。

{{< figure src="/posts/2018/06/14/figure1.3.png" title="图 1.3" caption="误差函数 `$ \eqref{eq:sum-square} $` 定义为每个数据点和函数 `$ y(x, \mathbf{w}) $` 的偏差（显示为为绿色竖线）的平方和（的一半）。" width="100%" >}}

我们可以通过选择合适的 `$ \mathbf{w} $` 最小化 `$ E(\mathbf{w}) $`，来解决曲线拟合的问题。因为误差函数是系数 `$ \mathbf{w} $` 的二次函数，它相对系数的导数线性于 `$ \mathbf{w} $` 中的元素，因此最小化误差函数有唯一解，记为 `$ \mathbf{w}^\star $`，它可以被解析的找到。结果多项式记为 `$ y(x, \mathbf{w}^\star) $`。

还有一个问题，我们该如何选择多项式的阶数 `$ M $`，如之后会看到，这是一个被称作 _模型比较_ 或 _模型选择_ 的问题。在图 1.4 中，我们展示了拟合 `$ M = 0, 1, 3, 9 $` 阶多项式的拟合结果。

{{< figure src="/posts/2018/06/14/figure1.4.png" title="图 1.4" caption="针对图 1.2 中的数据集，不同阶数 `$ M $` 多项式的拟合结果（红色曲线）。" width="100%" >}}

注意到常数（`$ M = 0 $`）和一阶（`$ M = 1 $`）多项式对数据拟合的很差，因此是函数 `$ \sin{(2\pi x)} $` 的不足表示。三阶（`$ M = 3 $`）多项式拟合给出函数 `$ \sin{(2\pi x)} $` 的最优拟合。当继续提高阶数（`$ M = 3 $`），我们得到对训练数据很棒的拟合。事实上，此时多项式穿过每个数据点，`$ E(\mathbf{w}^\star) = 0 $` 。但是，拟合的曲线剧烈震荡，是对函数 `$ \sin{(2\pi x)} $` 很差的表示，这种现象被称为 _过拟合_ 。

如之前说的，我们要做的是对新数据做出准确的预测，实现好的泛化。想要量化的看清 `$ M $` 的选择和泛化能力之间的关系，我们可以用生成训练集的方法，使用不同的噪声值，生成一个包含 `$ 100 $` 个数据点的独立测试集。对每个 `$ M $` 的值，我们使用 `$ \eqref{eq:sum-square} $` 计算训练集与测试集的残差 `$ E(\mathbf{w}^\star) $`。有时使用均方根函数（RMS）更加方便，定义为

<div>
\begin{equation}
E_{\mathrm{RMS}} = \sqrt{2E(\mathbf{w}^\star) / N}
\label{eq:rms}
\end{equation}
</div>

其中处以 `$ N $` 允许我们同等的比较不同大小的数据集，平方根保证了 `$ E_\mathrm{RMS} $` 与目标变量 `$ t $` 拥有相同尺度（与单位）。不同 `$ M $` 值下训练与测试集的均方根误差见图 1.5。测试集误差表示对新观测 `$ x $` 的预测 `$ t $` 的准确程度。从图 1.5 可以看出，较小的 `$ M $` 值会造成较大的测试误差，这是因为对应的多项式灵活度不足，不足以捕捉函数 `$ \sin{(2\pi x)} $` 的震荡。`$ M $` 的值在 `$ 3 \le M \le 8 $` 间时给出较小的测试集误差，从 `$ M = 3 $` 可以看出，此时多项式也足够表示生成函数 `$ \sin{(2\pi x)} $`。

{{< figure src="/posts/2018/06/14/figure1.5.png" title="图 1.5" caption="不同的 `$ M $` 值下，训练集与独立测试集的均方根误差（定义见 `$ \eqref{eq:rms} $`）。" width="100%" >}}

当 `$ M = 9 $` 时，如预料中的那样，训练集误差降为 `$ 0 $`，因为此时多项式包含 `$ 10 $` 个自由度（对应 `$ 10 $` 个系数 `$ w_0, \dots, w_9 $`），因此可以正好穿越 `$ 10 $` 个数据点。然而如图 1.4 所示，此时测试集误差急速增大，对应的函数 `$ y(x, \mathbf{w}^\star) $` 剧烈震荡。

这似乎自相矛盾，因为低阶多项式可以看作高阶多项式的特例。因此 `$ M = 9 $` 阶多项式至少可以生成和 `$ M = 3 $` 阶多项式一样好的结果。另外，可以认为函数 `$ \sin{(2 \pi x)}$` 能产生对新数据最好的预测（如之后会看到的，这的确是事实）。我们知道函数 `$ \sin{(2 \pi x)}$` 的幂级数展开包含所有阶次，因此可以期待当增加 `$ M $` 时，拟合的结果也会单调提升。

我们可以通过调查不同阶数下系数 `$ \mathbf{w}^\star $` 的值，获得对问题的一些直观理解。如表 1.1 所示，当 `$ M $` 增加时，系数的大小变得更大。特别的对 `$ M = 9 $` 阶多项式，为了完美通过每个数据点，系数变得很大或很小，函数在数据点间产生剧烈震荡（图 1.4）。直观来说，阶数越高，多项式的灵活性越强，也越容易拟合到目标值的噪音上。

{{< figure src="/posts/2018/06/14/table1.1.png" title="表 1.1" caption="多项式在不同的阶数下，系数 `$ \mathbf{w}^\star $` 的值。注意在阶数提升时，系数的值是如何迅速增加的。" width="100%" >}}

另一件很有意思的事，我们可以观察给定模型在不同大小数据集下的表现（如图 1.6）。可以看出，对给定复杂度的模型，随着数据集大小的增大，过拟合的问题逐渐得到缓解。另一种说法是，数据集越大，我们可以使用更复杂（即更灵活）的模型做拟合。一个粗糙的经验是，数据集的大小不应小于模型参数的某个整数（比如 5 或 10）倍。然而，如我们将在第三章看到的，参数数量未必能很好的刻画模型复杂度。

{{< figure src="/posts/2018/06/14/figure1.6.png" title="图 1.6" caption="使用 `$ M = 9 $` 阶多项式优化平方和误差函数的结果，左图使用 `$ N = 15 $` 个数据点的训练集，右图使用 `$ N = 100 $` 个数据点的训练集。可以看出增加数据集的大小减小了过拟合的风险。" width="100%" >}}

另外，根据训练集的大小限制模型参数的数量，多少让人有些不开心。无疑，根据待解决问题的复杂度，选取模型的复杂度看起来更合理。我们会看到，使用最小二乘法选取模型参数，实际上是 _最大似然估计_ （将在 1.2.5 节讨论）的一个特例，过拟合问题可以理解为最大似然估计的一个普遍属性。通过采用 _贝叶斯_ 方法，可以避免过拟合问题。我们将会看到，从贝叶斯角度来看，使用一个参数数量远多于数据点的模型不是什么难题。事实上，在贝叶斯模型中，_有效的_ 参数数量会自动根据数据集的大小调整。

然而，我们暂时继续使用现有的办法，考虑如何对大小有限的数据集应用相对复杂、灵活的模型。控制过拟合的一个办法是 _正则化_ ，具体来说，在误差函数 `$ \eqref{eq:sum-square} $` 的基础上加入一个惩罚项，防止系数变得过大。最简单的惩罚项是所有参数的平方和，修改后的误差函数如下

<div>
\begin{equation}
\tilde{E}(\mathbf{w}) = \frac{1}{2}\sum_{n = 1}^N{\{y(x_n, \mathbf{w}) - t_n}\}^2 + \frac{\lambda}{2} {\lVert \mathbf{w} \rVert}^2
\label{eq:sum-square-penalty}
\end{equation}
</div>

其中 `$ {\lVert \mathbf{w} \rVert}^2 = \mathbf{w}^\mathrm{T}\mathbf{w} = w_0^2 + w_1^2 + \dots + w_M^2 $`，系数 `$ \lambda $` 控制正则化项相对平方和误差项的重要程度。注意，大家经常在正则化项中省略`$ w_0 $`，因为若将其包含在内，会使得结果受到目标变量原点选择的影响（Hastic _et al._, 2001）。另一种解决方案是包含 `$ w_0 $`，但为其单独设置正则化项系数（我们会在 5.5.1 节详细讨论这个话题）。误差函数 `$ \eqref{eq:sum-square-penalty} $` 同样可以解析地获得最小值。在统计学文献中，这种技术被称作 _缩减_ 方法，因为减小了系数项的值。当使用二次正则化项时，这种曲线拟合方法被称作 _岭回归_ （Hoerl and Kennard, 1970）。在神经网络中，同样的方法被称作 _权值衰减_ 。

{{< figure src="/posts/2018/06/14/figure1.7.png" title="图 1.7" caption="在图 1.2 的数据集上，使用 `$ M = 9 $` 阶多项式优化误差函数 `$ \eqref{eq:sum-square-penalty} $` 的结果，两幅图分别设置正则化参数 `$ \lambda $` 为 `$ \ln{\lambda} = -18 $` 和 `$ \ln{\lambda} = 0 $`。没有正则化项（即 `$ \lambda = 0 $` 或 `$ \ln{\lambda} = -\infty $`） 的结果展示于图 1.4 右下角。" width="100%" >}}

图 1.7 展示了使用之前的数据集，并采用带正则化项的误差函数 `$ \eqref{eq:sum-square-penalty} $` 的拟合结果。可以看出，当 `$ \ln{\lambda} = -18 $` 时，过拟合得到了抑制，我们得到了一条更近似于 `$ \sin{(2 \pi x)} $` 函数的曲线。然而如图 1.7 中 `$ \ln{\lambda} = 0 $` 图所示， 当使用过大的 `$ \lambda $` 时，我们再次得到了很差的结果。表 1.2 中给出了几种情况下多项式的系数值，可以看出正则化对减小系数大小的作用。

{{< figure src="/posts/2018/06/14/table1.2.png" title="表 1.2" caption="不同正则化参数 `$ \lambda $` 下，`$ M = 9 $` 阶多项式拟合得到的系数值 `$ \mathbf{w}^\star $`，其中 `$ \ln{\lambda} = -\infty $` 对应不使用正则化，即对应图 1.4 右下角。可以看出随着 `$ \lambda $` 增大，系数的大小逐渐减小。" width="100%" >}}

可以进一步通过绘制训练集与测试集的均方根误差 `$ \eqref{eq:rms} $` 与 `$ \ln{\lambda} $` 的关系，看出正则化项对泛化误差的影响。如图 1.8 所示，`$ \lambda $` 实际上控制了模型的有效复杂度，因此影响了过拟合的程度。

{{< figure src="/posts/2018/06/14/figure1.8.png" title="图 1.8" caption="`$ M = 9 $` 阶多项式拟合的均方根误差对 `$ \ln{\lambda} $` 的关系图。" width="100%" >}}

模型复杂度是一个很重要的问题，在 1.3 节会展开讨论。这里我们简单总结一下，当通过最小化误差函数解决实际问题时，需要找到方法确定合适的模型复杂度。前述结果提供了一种简单手段，即把有效数据分成训练集（用于确定系数 `$ \mathbf{w} $`）与独立的 _验证集_ （也称作 _留出集_ ）用于选取模型复杂度（通过改变 `$ M $` 或 `$ \lambda $`）。然而在很多情况下，这会浪费宝贵的数据，我们需要寻找更为精巧的方法。

到现在为止，我们对多项式拟合的讨论大多停留于直觉。我们需要一套更有原则的方法来处理模式识别中的问题，因此接下来，我们将目光转向概率论，它将成为本书接下来几乎所有内容的基础，也为我们在多项式曲线拟合中介绍的概念提供深入的理解，以便能够将至扩展至更复杂的情形。

## 概率论

不确定性是模式识别领域的核心概念，它既来自观测中的噪声，也源自数据集大小的有限性。概率论为量化、操作不确定性提供了一套统一的框架，构成了模式识别的核心基础。通过和 1.5 节介绍决策轮结合，使得我们可以在给定的信息下做出最优预测，即使给定的信息可能不完整或含混不清。

我们通过一个简单的例子来介绍概率论的基本概念。想象有两个盒子，一个是红色的，一个是蓝色的。如图 1.9，在红色的盒子中放有 `$ 2 $` 个苹果和 `$ 6 $` 个橘子，蓝色的盒子中放有 `$ 3 $` 个苹果 和 `$ 1 $` 个橘子。现在随机挑选一个盒子，从盒子中随机选取一个水果，观察过后放回原来的盒子中。可以想象，如果在 `$ 40\% $` 的时间里选择红盒子，`$ 60\% $` 的时间里选择蓝盒子，重复这个过程足够多遍，拿到两种水果的机会最终是均等的。

{{< figure src="/posts/2018/06/14/figure1.9.png" title="图 1.9" caption="我们使用两个装有水果的彩色盒子（苹果显示为绿色，橘子显示为橙色）介绍概率论的基本概念。" width="100%" >}}

这个例子中，待选择的盒子是一个随机变量，记为 `$ B $`。这个随机变量有两个可能的取值：`$ r $`（对应选取红盒子）和 `$ b $`（对应选取蓝盒子）。类似的，选取的水果也是一个随机变量，记为 `$ F $`。它可以取值 `$ a $`（苹果）或 `$ o $`（橘子）。
